{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 5\n",
    "* Due: 11.2.2016 at noon\n",
    "* Max points: 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General rules\n",
    "* Send the source code or the written notes of your answers as an attachment to markus.hartikainen@jyu.fi before the due time\n",
    "* You will get feedback about your answers a week from the due date\n",
    "* From the second week on, the exercises will be given on the previous Wednesday's lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "For exercises 1-2, we study optimization problem\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min \\qquad & x_1^2+x_2^2 + x_3^3+(1-x_4)^2\\\\\n",
    "\\text{s.t.}\\qquad &x_1^2+x_2^2-1=0\\\\\n",
    "    &x_1^2+x_3^2-1=0\\\\\n",
    "    &x_1,x_2\\in\\mathbb R\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "1. **(2 points)** Use the SQP method to solve the above problem.\n",
    "2. **(2 points)** Use the Lagrangian method to solve the above problem.\n",
    "3. **(2 points)** Solve the problem\n",
    "$$\n",
    "\\begin{align}\n",
    "\\min   \\  & x_1^2 + x_2^2\\\\\n",
    "\\text{s.t. } & x_1 + x_2 \\geq 1.\n",
    "\\end{align}$$ by using just the optimality conditions.\n",
    "4. **(2 points)** Consider a problem\n",
    "$$\\begin{align}\n",
    "\\min   \\  & f(x)\\\\\n",
    "\\text{s.t. } & h_k(x)=0, \\text{ for all } k=1,\\dots,K,\n",
    "\\end{align}\n",
    "$$\n",
    "where all the functions are twice differentiable. Show, that the gradient of the augmented Lagrangian function is zero in the minimizer $x^*$ of the above problem. In other words, show that $\\nabla_xL_A(x^*,\\mu^*,\\varrho)=0$, where $\\mu^*\\in R^n$ is the corresponding optimal Lagrange multiplier vector.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def f_ex5(x):\n",
    "    return \\\n",
    "        x[0] ** 2 + x[1] ** 2 + x[2] ** 3 + (1 - x[3]) ** 2,\\\n",
    "        [],\\\n",
    "        [x[0] ** 2 + x[1] ** 2 - 1, x[0] ** 2 + x[2] ** 2 - 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import ad\n",
    "\n",
    "#if k=0, returns the gradient of lagrangian, if k=1, returns the hessian\n",
    "def diff_L(f, x, m, k):\n",
    "    #Define the lagrangian for given m and f\n",
    "    L = lambda x_: f(x_)[0] + (np.matrix(f(x_)[2]) * np.matrix(m).transpose())[0,0]\n",
    "    return ad.gh(L)[k](x)\n",
    "\n",
    "#Returns the gradients of the equality constraints\n",
    "def grad_h(f,x):\n",
    "    return  [ad.gh(lambda y: \\\n",
    "                   f(y)[2][i])[0](x) for i in range(len(f(x)[2]))] \n",
    "\n",
    "#Solves the quadratic problem inside the SQP method\n",
    "def solve_QP(f,x,m):\n",
    "    left_side_first_row = np.concatenate((\\\n",
    "        np.matrix(diff_L(f, x, m, 1)),\\\n",
    "        np.matrix(grad_h(f, x)).transpose()), axis = 1)\n",
    "    left_side_second_row = np.concatenate((\\\n",
    "        np.matrix(grad_h(f, x)),\\\n",
    "        np.matrix(np.zeros((len(f(x)[2]), len(f(x)[2]))))), axis = 1)\n",
    "    right_hand_side = np.concatenate((\\\n",
    "        -1 * np.matrix(diff_L(f, x, m, 0)).transpose(),\n",
    "        -np.matrix(f(x)[2]).transpose()), axis = 0)\n",
    "    left_hand_side = np.concatenate((\\\n",
    "                                    left_side_first_row,\\\n",
    "                                    left_side_second_row), axis = 0)\n",
    "    temp = np.linalg.solve(left_hand_side, right_hand_side)\n",
    "    return temp[:len(x)], temp[len(x):]\n",
    "    \n",
    "    \n",
    "\n",
    "def SQP(f,start,precision):\n",
    "    x = start\n",
    "    m = np.ones(len(f(x)[2]))\n",
    "    f_old = float('inf')\n",
    "    f_new = f(x)[0]\n",
    "    while abs(f_old - f_new) > precision:\n",
    "        f_old = f_new\n",
    "        (p, v) = solve_QP(f, x, m)\n",
    "        x = x + np.array(p.transpose())[0]\n",
    "        m = m + v\n",
    "        f_new = f(x)[0]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal is  [ 0.99267448 -0.12274973 -0.12274973  1.        ]\n",
      "Values of f(x) and constraints:  (0.99862059207569676, [], [0.00047012339004348647, 0.00047012339004348647])\n"
     ]
    }
   ],
   "source": [
    "from math import sqrt\n",
    "\n",
    "o = SQP(f_ex5, [0.5, -sqrt(0.75), -sqrt(0.75), 1], 0.001)\n",
    "print(\"Optimal is \", o)\n",
    "print(\"Values of f(x) and constraints: \", f_ex5(o))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def augmented_langrangian(f, x, mu, c):\n",
    "    second_term = float(numpy.matrix(mu) * numpy.matrix(f(x)[2]).transpose())\n",
    "    third_term = 0.5 * c * numpy.linalg.norm(f(x)[2]) ** 2\n",
    "    return f(x)[0] - second_term + third_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "import numpy\n",
    "def augmented_langrangian_method(f, start, mu0, c0, constraintNormAccuracy = 0.00001):\n",
    "    x_old = [float('inf')] * 2\n",
    "    x_new = start\n",
    "    mu = mu0\n",
    "    c = c0\n",
    "    while numpy.linalg.norm(f(x_new)[2]) > constraintNormAccuracy:\n",
    "        res = minimize(lambda x:augmented_langrangian(f, x, mu, c), x_new)\n",
    "        x_old = x_new\n",
    "        #mu = float(mu - numpy.matrix(c) * numpy.matrix(f(x_old)[2]).transpose())\n",
    "        \n",
    "        mu = mu - c * numpy.matrix(f(x_old)[2])\n",
    "        #print(mu)\n",
    "        x_new = res.x\n",
    "        #print(numpy.linalg.norm(f(x_new)[2]))\n",
    "        c = 2 * c\n",
    "    return x_new,c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "import numpy\n",
    "def penalty_function_method(f, start, c0, constraintNormAccuracy = 0.00001):\n",
    "    x_old = [float('inf')]*2\n",
    "    x_new = start\n",
    "    c = c0\n",
    "    mu = numpy.zeros(len(f(x_new)[2]))\n",
    "    while numpy.linalg.norm(f(x_new)[2]) > constraintNormAccuracy:\n",
    "        res = minimize(lambda x : augmented_langrangian(f, x, mu, c), x_new)\n",
    "        x_old = x_new\n",
    "        x_new = res.x\n",
    "        c = 2 * c\n",
    "    return x_new, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "initial_x = [0, 0, 0, 0]\n",
    "initial_mu = [0, 0]\n",
    "initial_c = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C at optimal:  4194304\n",
      "Optimal x:  [ -1.68422245e-08   1.00002368e+00  -1.00004147e+00   9.99999992e-01]\n",
      "Optimal f(x):  (-7.7063103568638885e-05, [], [4.7360642478677661e-05, 8.2947443986070013e-05])\n"
     ]
    }
   ],
   "source": [
    "result = augmented_langrangian_method(f_ex5, initial_x, initial_mu, initial_c, 0.0001)\n",
    "\n",
    "print(\"C at optimal: \", result[1])\n",
    "print(\"Optimal x: \", result[0])\n",
    "print(\"Optimal f(x): \", f_ex5(result[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C at optimal:  2097152\n",
      "Optimal x:  [  9.99999509e-01   2.39204391e-07   9.90253476e-04   1.00000001e+00]\n",
      "Optimal f(x):  (0.99999901893313459, [], [-9.8203790999118468e-07, -1.4360205247143654e-09])\n"
     ]
    }
   ],
   "source": [
    "result = penalty_function_method(f_ex5, initial_x, initial_c, 0.000001)\n",
    "\n",
    "print(\"C at optimal: \", result[1])\n",
    "print(\"Optimal x: \", result[0])\n",
    "print(\"Optimal f(x): \", f_ex5(result[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optimal seems to be [0, 1, -1, 1], f(x) = 0 with Augmented Lagrangian method. Penalty function method converges to different point [1, 0, 0, 1], f(x) = 1 for what ever reason (bug in code or AL uses different path to optimal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal suggested by Augmented Lagrangian:  (0, [], [0, 0])\n",
      "Optimal suggested by Penalty Method:  (1, [], [0, 0])\n"
     ]
    }
   ],
   "source": [
    "print(\"Optimal suggested by Augmented Lagrangian: \", f_ex5([0, 1, -1, 1]))\n",
    "print(\"Optimal suggested by Penalty Method: \", f_ex5([1, 0, 0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Augmented:  (array([ 0.5,  0.5]), 2)\n",
      "Penalty:  (array([ 0.49999618,  0.49999618]), 262144)\n"
     ]
    }
   ],
   "source": [
    "# TEST CODE FOR METHOD VALIDATION, optimal [0.5 0.5]\n",
    "\n",
    "def f_constrained2(x):\n",
    "    return sum([i**2 for i in x]),[],[sum(x)-1]\n",
    "\n",
    "print(\"Augmented: \", augmented_langrangian_method(f_constrained2,[0,0],1,1))\n",
    "print(\"Penalty: \", penalty_function_method(f_constrained2,[0,0],1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Exercise 5.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can figure out the relations of the varibles from the stationary rule:\n",
    "$$\n",
    "L(x,\\lambda,\\mu) = (x_1^2+x_2^2)-\\mu_1(x_1+x_2-1),\n",
    "$$\n",
    "then\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_x L(x,\\lambda,\\mu) &= (2x_1+2x_2)-\\mu_1(1+1)\\\\\n",
    "&= 2(x_1+x_2)-2\\mu_1\n",
    "\\end{align}\n",
    "$$\n",
    "so\n",
    "$$\n",
    "\\mu_1=x_1+x_2\n",
    "$$\n",
    "for the complementary rule $\\mu_1(x_1+x_2-1)=0$ to hold, we see that $x_1=x_2=0.5$ (making $x_1+x_2-1=0$) and thus $\\mu_1=x_1+x_2=1$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In essence\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla_x L(x,\\lambda,\\mu) &= 0\n",
    "\\end{align}\n",
    "$$\n",
    "is the balance of\n",
    "$$\n",
    "\\begin{align}\n",
    "\\nabla f(x) &= \\sum_{i=1}^n\\mu_i\\nabla h_i(x)\n",
    "\\end{align}\n",
    "$$\n",
    "So at optimal $x^*$ the Lagrange multipliers $\\mu_i$ balance the gradients of constraints to the gradient of the objective function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
